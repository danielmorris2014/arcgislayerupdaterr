Update the ArcGISLayerUpdater Streamlit app to resolve the persistent 'Error creating layer: 'dict' object has no attribute 'to_csv' error that occurs when uploading a zip file (e.g., 'MST_BURIED.zip') containing shapefiles to create a new layer in ArcGIS Online. The app uses the ArcGIS API for Python and GeoPandas, and the error arises during the layer creation step, indicating that a dictionary is being passed instead of a pandas DataFrame to a to_csv call. Previous attempts with WKT conversion, GeoDataFrame validation, and fallback methods have not fully resolved the issue. Implement the following targeted solution:

Objectives:

Eliminate the to_csv dependency to avoid the error.
Ensure the data remains a pandas DataFrame throughout the process.
Add detailed debugging to pinpoint where the data type changes.
Steps to Implement:

Shapefile Extraction and Initial Validation:
Extract the uploaded .zip file to a temporary directory (e.g., 'temp_shapefile') using zipfile.ZipFile.
Verify the presence of .shp, .shx, and .dbf files. If any are missing, raise an error with st.error.
Load and Validate Shapefile Data:
Use geopandas.read_file() to load the .shp file into a GeoDataFrame.
Check if the result is a gpd.GeoDataFrame with isinstance(gdf, gpd.GeoDataFrame). If not, log the failure and display st.error.
If the GeoDataFrame is empty (gdf.empty), raise a ValueError with a clear message.
Convert Geometry and Create DataFrame:
Add a 'wkt_geometry' column using gdf['geometry'].apply(lambda geom: geom.wkt if geom else None) to convert geometry to WKT.
Create a pandas DataFrame by dropping the original geometry column with df = pd.DataFrame(gdf.drop(columns='geometry')).
Verify the result is a pd.DataFrame with isinstance(df, pd.DataFrame). If not, raise a TypeError.
Direct Layer Creation Without CSV:
Use gis.content.import_data(df, title=layer_title) to create the layer directly from the DataFrame, bypassing to_csv entirely.
If import_data fails (e.g., due to data format issues), log the error and attempt a fallback by exporting to a CSV temporarily and importing it with gis.content.add().
Granular Debugging:
Add logging at every step (e.g., 'Extracted zip', 'Loaded GeoDataFrame', 'Converted to DataFrame') to app_log.txt.
Include a st.checkbox("Enable Debug Mode") to display intermediate data types and samples (e.g., st.write(type(df), df.head())) in the app.
Log the exact data structure before calling import_data to catch where it might become a dictionary.
Error Handling and Cleanup:
Wrap the entire process in a try-except block to catch all exceptions, displaying them with st.error.
Clean up the temporary directory with shutil.rmtree in a finally block.

import streamlit as st
import geopandas as gpd
import pandas as pd
from arcgis.gis import GIS
import zipfile
import os
import logging
import shutil

# Configure logging
logging.basicConfig(filename='app_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')

st.title("ArcGIS Layer Updater")
uploaded_file = st.file_uploader("Upload a shapefile ZIP (e.g., MST_BURIED.zip)", type="zip")
debug_mode = st.checkbox("Enable Debug Mode")

if uploaded_file:
    try:
        # Extract zip
        temp_dir = "temp_shapefile"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        with zipfile.ZipFile(uploaded_file, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        logging.info("Extracted zip file")
        if debug_mode:
            st.write("Extracted files:", os.listdir(temp_dir))

        # Find .shp file
        shp_file = next((f for f in os.listdir(temp_dir) if f.endswith('.shp')), None)
        if not shp_file:
            raise FileNotFoundError("No .shp file found in zip")
        shp_path = os.path.join(temp_dir, shp_file)
        logging.info(f"Found shapefile: {shp_file}")

        # Load GeoDataFrame
        gdf = gpd.read_file(shp_path)
        logging.info(f"Loaded data type: {type(gdf)}")
        if not isinstance(gdf, gpd.GeoDataFrame):
            raise TypeError("Expected GeoDataFrame, got {type(gdf)}")
        if gdf.empty:
            raise ValueError("Shapefile contains no data")
        if debug_mode:
            st.write("GeoDataFrame type:", type(gdf))
            st.write("GeoDataFrame sample:", gdf.head())

        # Convert geometry to WKT
        gdf['wkt_geometry'] = gdf['geometry'].apply(lambda geom: geom.wkt if geom else None)
        df = pd.DataFrame(gdf.drop(columns=['geometry']))
        logging.info(f"Converted data type: {type(df)}")
        if not isinstance(df, pd.DataFrame):
            raise TypeError(f"Expected DataFrame, got {type(df)}")
        if debug_mode:
            st.write("DataFrame type:", type(df))
            st.write("DataFrame sample:", df.head())

        # Create layer
        gis = GIS("https://www.arcgis.com", "your_username", "your_password")  # Replace with credentials
        layer_title = f"New_Layer_{uploaded_file.name.split('.')[0]}"
        logging.info(f"Data type before import_data: {type(df)}")
        if debug_mode:
            st.write("Data before import:", type(df), df.head())
        
        feature_layer = gis.content.import_data(df, title=layer_title)
        logging.info(f"Layer created, type: {type(feature_layer)}")
        st.success(f"Layer created! Access it: {gis.url}/home/item.html?id={feature_layer.id}")

    except Exception as e:
        st.error(f"Error creating layer: {str(e)}")
        logging.error(f"Layer creation error: {str(e)}")
        if debug_mode:
            st.write("Error details:", str(e))

    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            logging.info("Cleaned up temp directory")
			
			Replace 'your_username' and 'your_password' with your ArcGIS credentials.
Upload MST_BURIED.zip with debug mode enabled.
Check app_log.txt and Streamlit output to trace the data type at each step.
Expected Outcome:

The app creates the layer without the to_csv error by using import_data directly.
If the error persists, the debug logs will show where the data becomes a dictionary, allowing for targeted fixes.